{"categories":[{"title":"Kỹ Thuật","uri":"https://lhsang.github.io/categories/k%E1%BB%B9-thu%E1%BA%ADt/"},{"title":"Trải Nghiệm","uri":"https://lhsang.github.io/categories/tr%E1%BA%A3i-nghi%E1%BB%87m/"}],"posts":[{"content":"\nVai trò của dữ liệu thì chúng ta không cần bàn luận nữa. Hôm nay mình sẻ chia sẻ một vài phương pháp và khó khăn khi cào dữ liệu (crawl data) từ những phương pháp, công cụ mình đã ứng dụng và một số vấn đề gặp phải trong quá trình làm luận văn. (có 2 khái niệm là data crawling và data scraping nhưng mình chỉ nói nôn na là thu thập dữ liệu, các bạn có thể đọc thêm để phân biệt)\nCào ở dữ liệu ở đây mình đề cập là xây dựng một tool để đi thu thập dữ liệu ở một số website mà chúng ta cần, đơn thuần cũng chỉ là gởi một request HTTP/HTTPS trực tiếp hoặc dùng một browser driver truy cập đến trang đích để lấy nội dung trang về rồi tiến hành parse nội dung để lấy dữ liệu. Hầu hết các trang web đều không thể ngăn cấm việc bot ghé thăm vì trong đó có bot của các search engine như google, bing,\u0026hellip; tuy nhiên họ cũng không thể nào cho phép truy cập thỏa mái vì điều này làm giảm hiệu năng hoặc thậm chí sập cả server. Chính vì vậy đa số các website đều quy định và tìm cách hạn chế, ngăn chặn việc crawler quá mức, thậm chí có cả bẫy (traps) để chặn crawler khiến cho việc thu thập dữ liệu của chúng ta cũng gặp không ít khó khăn.\nTrước hết mình sẽ giới thiệu qua một số tool/lib/framework phổ biến để thực hiện thu thập dữ liệu, mặc dù việc cào dữ liệu chỉ đơn thuần là gởi request để lấy nội dung nhưng bạn cũng nên dùng thư viện sẵn có để thu thập (trừ khi các bạn muốn phát triển 1 thư viện mới) thay vì tự gởi request bằng thư viện HTTP trong các ngôn ngữ sẵn có vì các bạn sẽ phải handle rất nhiều thứ từ đa luồng, kiểm soát request, parsing,\u0026hellip; rất tốn thời gian để viết lại. Sử dụng các thư viện sẵn có thì bạn cũng không cần lo về khả năng mở rộng, tùy biến của mình, các thư viện đa số đã thiết kế theo kiến trúc mở cung cấp các middlewares, pipelines cho người dùng tùy biến.\nScrapy Selenium Cheerio Puppeteer Advantages - Bất đồng bộ chính là thứ giúp cho scrapy có hiệu năng tuyệt vời. Sử dụng rất ít RAM, CPU. Tài liệu đầy đủ, plugins rất nhiều. Thiết kế theo một kiến trúc khá tốt, dev có thể dễ dàng tùy chỉnh middlewares, pipelines và thêm các function tùy ý. - Dễ tiếp cận cho beginer. Có thể lấy được nội dung trang render bằng javascript. Có thể thao tác như một người dùng - Hỗ trợ parse DOM giống như jquery. Bất đồng bộ, khá nhanh - Giống như selenium, puppeteer cũng sử dụng browser driver để lấy nội dung trang nên có thể lấy được nội dung các trang render bằng js và thao tác như một người dùng. Disadvantages - Hơi phức tạp cho beginer. Quá dư thừa chức năng nếu chỉ dùng cho 1 project nhỏ và đơn giản. Không thể lấy nội dung trang được render bằng js, tuy nhiên chúng ta có thể kết hợp với Splash. - Tiêu tốn nhiều tài nguyên vì nó cần đến một browser driver (tưởng tượng như bạn mở 1 tab chrome thì sẽ tốn bao nhiêu RAM, CPU). Rất chậm, nên selenium không phù hợp với một project cào dữ liệu lớn. - Chỉ đơn thuần là trình phân tích (parser). Không thể lấy nội dung trang được render bằng js - Tiêu tốn RAM, CPU. Chậm Hầu hết các ngôn ngữ lập trình đều có những thư viện hỗ trợ thu thập dữ liệu, tuy nhiên Python sẽ là ngôn ngữ mà mình gợi ý các bạn nên chọn cho dự án thu thập dữ liệu của mình, Scrapy là một framework trong python hỗ trợ thu thập dữ liệu cực mạnh, hơn nữa python có hỗ trợ khá nhiều thư viện để xử lý dữ liệu.\nNhập môn thu thập dữ liệu, kiến trúc chung của một crawler sẽ gồm 3 phần: queue chứa URL, downloader, parser. Luồng hoạt động sẽ diễn ra như sau:\nCung cấp một URL để start (hoặc một list URL) Downloader tiến hành tải xuống nội dung trang Parser rút trích dữ liệu (store nếu cần) và đồng thời rút trích ra URL kế tiếp và thêm vào queue URL Quá trình sẽ lặp lại cho đến hết URL trong queue Để làm quen với việc thu thập dữ liệu, mình khuyên các bạn nên làm qua 10 bài thực hành trên trang Scrapingclub (Learn Web Scraping Using Python For Free) theo bất kỳ thư viện hỗ trợ thu thập dữ liệu nào bạn muốn, solutions thì mình cũng đã làm từ lúc mình học cơ bản https://github.com/lhsang/Spiders (sử dụng scrapy) các bạn có thể tham khảo.\nVề vấn đề thu thập thông tin để tránh vi phạm các tiêu chuẩn, quy định thì các bạn nên phân tích file http://domain.com/robots.txt trước khi cào, file robots.txt sẽ định quy cho biết Crawler của bạn được truy cập những trang nào, không được truy cập trang nào và thời gian delay mỗi request. Ngoài ra user-agent là một header bạn có thể tùy ý, tuy nhiên về vấn đề đạo đức nghề nghiệp các bạn không nên fake giá trị này.\nPhần tiếp theo mình sẽ nói về một số khó khăn đã gặp và giải pháp trong quá trình thu thập dữ liệu phục vụ cho luận văn tốt nghiệp:\nCác trang hạn chế số lượng truy cập trong một khoảng thời gian: để tránh việc một IP làm quá tải server, các trang thường phải giới hạn số request của một IP trong một phút (thường sẽ họ sẽ cho biết thời gian deplay mỗi request trong file robots.txt), có một số lượng lớn trang cần thu thập thì môi phút vài request thì quá ít cho nhu cầu của mình và thậm chí nếu gửi nhiều hơn quy định, crawler còn có thể bị ban trong vài phút. Giải pháp là mình đã sử dụng thêm proxy server, giá proxy cũng không quá mắc (tầm $0.4 IP/tháng, khuyên các bạn không nên mua của các nhà cung cấp từ Việt Nam - người từng trải), mỗi request mình sẽ đổi IP liên tục thì sẽ tránh được ban IP. Nếu không cần thiết phải đổi IP thì ban có thể tăng thời gian DELAY_REQUEST để cách biệt các request, giảm CONCURRENT_REQUESTS (số request đồng thời) và có thể cheat nhẹ user-agent bất chấp đạo đức :)) Nội dung trang render bằng Javascript: vì nhu cầu mình crawl nhiều nên mình chọn Scrapy, tuy nhiên scrapy lại không hỗ trợ lấy dữ liệu ở các trang render bằng js (Ajax, React,\u0026hellip;) để biết rằng trang này có render bằng js ra nội dung không, bạn chỉ cần Ctrl+U rồi xem có nội dung html như hiển thị không, hay chỉ là một thẻ body rồi js chèn vào sau. Như đã đề cập scrapy có thể sử dụng kèm với headless browser như Splash để chờ trang web render ra nội dung và cookie, việc này làm tốn thêm một ít thời gian chờ nhưng vẫn nhanh hơn các lựa chọn khác scrapy. Cấu trúc trang thay đổi: đôi lúc sites thay đôỉ cấu trúc (HTML tags thay đổi), chúng ta phải xác định đúng thẻ thì mới lấy được dữ liệu. Nên cài đặt cho crawler cơ chế phát hiện cấu trúc thay đổi và thông báo để chúng ta biết và chỉnh sửa hoặc áp dụng cấu trúc mới để parse đúng dữ liệu. Required đủ cookie, headers: đa số các trang đều yêu cầu một số headers và cookie là phải có trong request, chúng ta có thể dùng Chrome DevTools để xem các header và cookie này, tuy nhiên ở nhiều trang headers, cookie này không được set ngay từ đầu mà trong quá trình request sẽ gởi request đến một URL khác để lấy thông tin headers, cookie hoặc ngay địa chỉ URL của bạn nhưng lần đầu là để lấy headers, cookie lần sau mới lấy nội dung (trường hợp này, nếu không có đủ headers server sẽ buộc crawler request vô hạn). Giải pháp là chúng ta phải biết quan sát mà lấy headers, cookie phù hợp để gắn vào request thôi =)) Bất đồng bộ và multithread vẫn chưa đủ: mỗi ngày mình phải gởi hơn vài chục triệu request, scrapy hỗ trợ bất đồng bộ, tuy nhiên một process chưa đủ đáp ứng nhu cầu lớn như vậy trong một ngày, giải pháp là tăng số process lên (có thể cho chạy ở nhiều server), liên quan đến vấn đề xử lý song song các bạn phải tính toán để chia URL ra cho các process hợp lý nhất. Lưu trữ dữ liệu: tùy nhu cầu bạn sẽ chọn hệ quản trị cơ sở dữ liệu phù hợp, với project của mình vì nhu cầu lưu trữ lớn (hơn 11 triệu record mỗi ngày đổ vào database) và cần tốc độ truy vẫn nhanh thì mình chọn mongodb. MongoDB thuộc loại NoSQL lưu trữ theo dạng document (BSON), schema linh hoạt, không phải join, kết hợp với đánh indexes thì tốc độ khá là nhanh (dữ liệu 1 tỷ records mình có thể truy vấn dưới 1s) hơn nữa mongodb còn thiết kế để đáp ứng nhu cầu phân tán nên có thể sharding hay replication để mở rộng, tăng hiệu năng và đảm bảo tính available cho hệ thống. Vấn đề lưu xuống liên tục cũng có thể quá tải database, bạn có thể cache tạm một nơi rồi insert_many thay vì insert_one. Trên là những chia sẻ mà mình rút trích được từ những vấn đề gặp phải trong quá trình làm luận văn mà mình gặp phải, ngoài ra vẫn còn kha khá issues mà mình chưa gặp phải nữa nếu biết các bạn có thể chia sẻ thêm dưới phần bình luận để cùng tranh luận, cảm ơn các bạn.\n","id":0,"section":"posts","summary":"\u003cp\u003e\u003cimg src=\"/img/posts/technique/crawl/crawl.jpg\" alt=\"Crawl data\"\u003e\u003c/p\u003e\n\u003cp\u003eVai trò của dữ liệu thì chúng ta không cần bàn luận nữa. Hôm nay mình sẻ chia sẻ một vài phương pháp và khó khăn khi cào dữ liệu (crawl data) từ những phương pháp, công cụ mình đã ứng dụng và một số vấn đề gặp phải trong quá trình làm luận văn. (có 2 khái niệm là data crawling và data scraping nhưng mình chỉ nói nôn na là thu thập dữ liệu, các bạn có thể đọc thêm để phân biệt)\u003c/p\u003e","tags":["Technique","crawler","thesis"],"title":"Crawl data - cào dữ liệu có gì khó?","uri":"https://lhsang.github.io/posts/technique/scraping-data-from-websites/","year":"2020"},{"content":"\nGiới thiệu về Full-Text Search Có lẽ chúng ta đều biết và sử dụng qua một kỹ thuật tìm kiếm rất cơ bản, đó là thông qua câu lệnh LIKE của SQL.\nSELECT column_name(s) FROM table_name WHERE column_name LIKE pattern; Sử dụng LIKE, chúng ta chỉ tìm kiếm ở column đã định trước, do đó không gian tìm kiếm bị giới hạn hơn. Về tốc độ, câu lệnh LIKE cũng tương đương với lệnh chúng ta matching pattern cho từng chuỗi của từng rows của column tương ứng thì bạn cũng hiểu tốc độ sẽ như thế nào rồi. Kết quả tìm kiếm có độ nhiễu cao và gặp vấn đề về từ đồng nghĩa.\nMột giải pháp tìm kiếm đầy đủ, nhanh hơn và linh hoạt hơn đó là Full-text search. Full-text search (FTS) là một kỹ thuật tìm kiếm kết quả trên cơ sở dữ liệu chứa \u0026ldquo;toàn bộ\u0026rdquo; các kí tự (text) của một hoặc một số tài liệu, bài báo,\u0026hellip;(document), hoặc là của website.\nFull-text search trong PostgreSQL PostgreSQL (thời điểm viết bài latest released version là 12.2) có các toán tử ~, ~*, LIKE, ILIKE để tìm kiếm cho kiểu dữ liệu văn bản, tuy nhiên như đề cập ở trên thì các phương pháp này chó ra kết quả không như mong đợi và thời gian query lâu.\nFTS trong postgreql xuất phát từ ý tưởng tiền xử lý (preprocess) document ở thời điểm index để tiết kiệm thời gian cho sau này query (cho quá trình tìm kiếm). Tiền xử lý sẽ bao gồm:\nParsing document to lexemes: rất hữu ích để phân biệt các loại lexemes (từ vựng) vì các loại lexemes khác nhau có thể được xử lý khác nhau, ví dụ: chữ số, chữ, từ phức, địa chỉ email,\u0026hellip; Apply các quy tắc ngôn ngữ để chuẩn hóa lexeme thành dạng nguyên mẫu, ví dụ: see, saw, seen đều đưa về dạng nguyên mẫu là see để lưu trữ. Stopword - những từ phổ biến, xuất hiện hầu hết trong mỗi document, tuy nhiên không thể phân biệt được gì từ chúng, nên postgresql cũng có các bộ từ điển và tự động loại bỏ stopword trước khi lưu. SELECT to_tsvector(\u0026#39;english\u0026#39;,\u0026#39;in the list of stop words\u0026#39;); -- english: dùng bộ từ điển tiếng anh (default english) ---------------------------- \u0026#39;list\u0026#39;:3 \u0026#39;stop\u0026#39;:5 \u0026#39;word\u0026#39;:6 Store document đã tiền xử lý theo một cách tối ưu để tìm kiếm nhanh hơn. Ví dụ biểu diễn tài liệu dưới dạng một mảng các từ vựng được sắp xếp. Lưu trữ vị ví xuất hiện, tần suất xuất hiện nhằm để xếp hạng (Ranking). PostgreSQL là một cơ sở dữ liệu có thể mở rộng, nên nó có thêm datatype mới là tsvector dùng cho tiền xử lý và lưu tài liệu, tsquery dùng để query. Và thêm operator @@ được defined cho datatype này. Và đương nhiên FTS cũng có thể được tăng tốc khi đánh index.\nText search functions 1. Parsing documents: tsvector to_tsvector([ config regconfig, ] document text)\ntsvector là một kiểu dữ liệu dạng document được dùng để tối ưu hóa full text search. chuỗi được phân tách và sắp xếp lại theo thứ tự từ điển, các từ trùng lặp chỉ lưu trữ một lần, đồng thời hàm có sử dụng dictionary (english - default) để loại bỏ những từ ngữ không quan trọng (mạo từ, tobe, \u0026hellip;) và chuyển về dạng nguyên mẫu để lưu trữ tiện cho việc tìm kiếm. Kèm theo đó là vị trí xuất hiện trong câu.\nSELECT to_tsvector(\u0026#39;english\u0026#39;, \u0026#39;A task was canceled.\u0026#39;); ----------------------- \u0026#39;cancel\u0026#39;:4 \u0026#39;task\u0026#39;:2 Ví dụ trên chúng ta thấy câu được tác thành các từ riêng biệt, được sắp xếp theo thứ tự từ điển, kèm theo đó là vị trí xuất hiện trong câu và các dấu câu, stopword bị lọai bỏ (theo từ điển tiếng anh).\n2. Parsing queries: tsquery to_tsquery([ config regconfig, ] querytext text)\ntsquery là một kiểu dữ liệu cho các truy vấn văn bản với sự hỗ trợ của toán tử boolean: \u0026amp; (and), | (or). Tsquery bao gồm các lexemes(từ vựng) và các toán tử boolean ở giữa.\nSELECT plainto_tsquery(\u0026#39;english\u0026#39;, \u0026#39;The Fat Rats\u0026#39;); ----------------- \u0026#39;fat\u0026#39; \u0026amp; \u0026#39;rat\u0026#39; SELECT to_tsquery(\u0026#39;english\u0026#39;, \u0026#39;The \u0026amp; Fat \u0026amp; Rats\u0026#39;); --------------- \u0026#39;fat\u0026#39; \u0026amp; \u0026#39;rat\u0026#39; SELECT to_tsquery(\u0026#39;english\u0026#39;, \u0026#39;The \u0026amp; Fat | Rats\u0026#39;); --------------- \u0026#39;fat\u0026#39; | \u0026#39;rat\u0026#39; 3. Một số hàm khác. Function Return Type Description Example Result array_to_tsvector(text[]) tsvector convert mảng lexemes thành tsvector array_to_tsvector(\u0026rsquo;{fat,cat,rat}\u0026rsquo;::text[]) \u0026lsquo;cat\u0026rsquo; \u0026lsquo;fat\u0026rsquo; \u0026lsquo;rat\u0026rsquo; plainto_tsquery([ config regconfig , ] query text) tsquery hàm tsquery bỏ qua dấu câu plainto_tsquery(\u0026rsquo;english\u0026rsquo;, \u0026lsquo;The Fat Rats\u0026rsquo;) \u0026lsquo;fat\u0026rsquo; \u0026amp; \u0026lsquo;rat\u0026rsquo; setweight(vector tsvector, weight \u0026ldquo;char\u0026rdquo;) tsvector assign weight cho các phần tử trong vector setweight(\u0026lsquo;fat:2,4 cat:3 rat:5B\u0026rsquo;::tsvector, \u0026lsquo;A\u0026rsquo;) \u0026lsquo;cat\u0026rsquo;:3A \u0026lsquo;fat\u0026rsquo;:2A,4A \u0026lsquo;rat\u0026rsquo;:5A get_current_ts_config() regconfig get config măc định get_current_ts_config() english ts_rank([ weights float4[], ] vector tsvector, query tsquery [, normalization integer ]) float4 tính điểm ranking ts_rank(textsearch, query) 0.818 \u0026hellip; Mình chỉ đề cập vài hàm hay dùng, để tìm hiểu nhiều hơn mời các bạn đọc tiếp tại trang chủ postgres\nFTS operator Full text search trong postgresql sử dụng toán tử @@ cho 2 loại dữ liệu tsvector và tsquery. FTS operator còn hỗ trợ các kiểu dữ liệu như text và varchar cho phép setup full-text search đơn giản (nhưng không hỗ trợ ranking). tsvector @@ tsquery\ntsquery @@ tsvector\ntext|varchar @@ text|tsquery\nToán tử @@ return TRUE nếu tsvector chứa tsquery.\nSELECT to_tsvector(\u0026#39;a fat cat sat on a mat - it ate a fat rats\u0026#39;) @@ to_tsquery(\u0026#39;The \u0026amp; Fat \u0026amp; Rats\u0026#39;); ------- t SELECT to_tsvector(\u0026#39;a fat cat sat on a mat - it ate a fat rats\u0026#39;) @@ to_tsquery(\u0026#39;The \u0026amp; Fat \u0026amp; Dogs\u0026#39;); -------- f SELECT \u0026#39;a fat cat sat on a mat - it ate a fat rats\u0026#39; @@ to_tsquery(\u0026#39;The \u0026amp; Fat \u0026amp; rat\u0026#39;); ------- t SELECT \u0026#39;a fat cat sat on a mat - it ate a fat rats\u0026#39; @@ \u0026#39;The \u0026amp; Fat \u0026amp; rat\u0026#39;; ------- t Toán tử || để nối 2 vector. SELECT to_tsvector(\u0026#39;xin chao\u0026#39;) || to_tsvector(\u0026#39;hello world\u0026#39;) --------------- \u0026#39;chao\u0026#39;:2 \u0026#39;hello\u0026#39;:3 \u0026#39;world\u0026#39;:4 \u0026#39;xin\u0026#39;:1 Các toán tử khác (Nguồn: Text Search Functions and Operators) Operator Return Type Description Example Result @@ boolean tsvector matches tsquery ? to_tsvector(\u0026lsquo;fat cats ate rats\u0026rsquo;) @@ to_tsquery(\u0026lsquo;cat \u0026amp; rat\u0026rsquo;) t @@@ boolean deprecated synonym for @@ to_tsvector(\u0026lsquo;fat cats ate rats\u0026rsquo;) @@@ to_tsquery(\u0026lsquo;cat \u0026amp; rat\u0026rsquo;) t || tsvector concatenate tsvectors \u0026lsquo;a:1 b:2\u0026rsquo;::tsvector || \u0026lsquo;c:1 d:2 b:3\u0026rsquo;::tsvector \u0026lsquo;a\u0026rsquo;:1 \u0026lsquo;b\u0026rsquo;:2,5 \u0026lsquo;c\u0026rsquo;:3 \u0026rsquo;d\u0026rsquo;:4 \u0026amp;\u0026amp; tsquery AND tsquerys together \u0026lsquo;fat | rat\u0026rsquo;::tsquery \u0026amp;\u0026amp; \u0026lsquo;cat\u0026rsquo;::tsquery ( \u0026lsquo;fat\u0026rsquo; | \u0026lsquo;rat\u0026rsquo; ) \u0026amp; \u0026lsquo;cat\u0026rsquo; || tsquery OR tsquerys together \u0026lsquo;fat | rat\u0026rsquo;::tsquery || \u0026lsquo;cat\u0026rsquo;::tsquery ( \u0026lsquo;fat\u0026rsquo; | \u0026lsquo;rat\u0026rsquo; ) | \u0026lsquo;cat\u0026rsquo; !! tsquery negate a tsquery !! \u0026lsquo;cat\u0026rsquo;::tsquery !\u0026lsquo;cat\u0026rsquo; \u0026lt;-\u0026gt; tsquery tsquery followed by tsquery to_tsquery(\u0026lsquo;fat\u0026rsquo;) \u0026lt;-\u0026gt; to_tsquery(\u0026lsquo;rat\u0026rsquo;) \u0026lsquo;fat\u0026rsquo; \u0026lt;-\u0026gt; \u0026lsquo;rat\u0026rsquo; @\u0026gt; boolean tsquery contains another ? \u0026lsquo;cat\u0026rsquo;::tsquery @\u0026gt; \u0026lsquo;cat \u0026amp; rat\u0026rsquo;::tsquery f \u0026lt;@ boolean tsquery is contained in ? \u0026lsquo;cat\u0026rsquo;::tsquery \u0026lt;@ \u0026lsquo;cat \u0026amp; rat\u0026rsquo;::tsquery t Ranking search results Ranking nhằm mục đích đo lường mức độ phù hợp của document đối với một query cụ thể. Postgres cung cấp 2 hàm ranking:\nRanking dựa trên tần số của các từ được match ts_rank([ weights float4[], ] vector tsvector, query tsquery [, normalization integer ]) returns float4\nCũng tương tự như ts_rank tuy nhiên hàm còn tính theo mật độ bao phủ của input trong document ts_rank_cd([ weights float4[], ] vector tsvector, query tsquery [, normalization integer ]) returns float4\nVí dụ:\nSELECT ts_rank( to_tsvector(\u0026#39;name lastname name lastname\u0026#39;), to_tsquery(\u0026#39;name \u0026amp; lastname\u0026#39;)); --- ts_rank: 0,3400053* SELECT ts_rank_cd( to_tsvector(\u0026#39;name lastname name lastname\u0026#39;), to_tsquery(\u0026#39;name \u0026amp; lastname\u0026#39;)); --- ts_rank_cd: 0,3 SELECT ts_rank_cd( to_tsvector(\u0026#39;name lastname abc xyz name lastname\u0026#39;), to_tsquery(\u0026#39;name \u0026amp; lastname\u0026#39;)); --- ts_rank_cd: 0,233* Cả 2 hàm trên đều, tham số weights là optional, các nhãn {D-weight, C-weight, B-weight, A-weight}\nnếu không chỉ định tham số weights thì trọng số mặc định là:\n{0.1, 0.2, 0.4, 1.0}\nIndexes Có 2 loại index có thể dùng để tăng tốc full-text search trong postgres là: GIN và GIST\nGin (Generalized Inverted Indexes) Loại index này sẽ hữu ích khi một chỉ mục phải ánh xạ tới nhiều giá trị trong cùng một bản ghi, trong khi đó B-Tree lại chỉ được tối ưu hóa nếu như một bản ghi chỉ có một giá trị khóa duy nhất. Chỉ mục GIN cũng rất hữu ích trong việc đánh chỉ mục các giá trị là mảng, hoặc khi phải thực hiện các truy vấn là full-text search.\nGiST - Generalized Search Tree: Loại index này cho phép bạn xây dựng cấu trúc cây cân bằng chung, và có thể được sử dụng cho nhiều loại so sánh ngoài hai loại so sánh bằng và so sánh phạm vi. Chỉ mục này cũng được sử dụng để đánh chỉ mục cho các kiểu dữ liệu hình học, cũng như thực hiện full-text search.\nVậy chọn GIN hay GiST để dùng?\nGIN tốn thời gian đánh index lâu hơn tuy nhiên lại tìm kiếm nhanh hơn GiST (3 lần) GIN update cũng lâu hơn (vừa phải) so với GiST, tuy nhiên sẽ lâu hơn tầm 10 lần nếu vô hiệu hóa fast-update. Chi phí về không gian của GIN cũng lớn hơn 2-3 lần so với GiST Bạn có thể bỏ qua phần phân tích sau nếu thấy bài đã đủ dài và đủ rối :))\nPhân tích một chút về BTree và GIN index:\nTrên là B-Tree index (thực ra BTree cũng có 2 loại là B-Tree và B+Tree nhưng ở đây mình giả sử B-Tree có trúc như vậy). Index bao gồm các trang, mỗi trang bao gồm nhiều index tuple và free space. Index tuple là cấu trúc bao gồm:\nkey datum: gía trị của cột được đánh index (column). Trên một trang lá (leaf page), key datum chính là con trỏ tham chiếu đến heap tuple (table row). Còn các page ở trong thì datatum là con trỏ chỉ đến các cây con được tham chiếu. tuple identifier (TID) - hay còn gọi là item pointer: thông tin về địa chỉ vật lý (địa chỉ trên disk) của index tuple hoặc heap tuple được tham chiếu (mỗi row chỉ tham chiếu chính xác bởi một TID) Mỗi row của table (table đánh index) đều phải được insert một tuple index vào cây, ngay cả khi một row khác có cùng giá trị khóa (key) đã tồn tại trên đó. Việc insert như vậy sẽ gây ra sự phân tách trang (số lượng entry trong 1 page đầy), ảnh hưởng đến trang khác và chiều cao của cây.\nGIN index\nCấu trúc các trang trong (internal pages) của GIN và B-tree không có gì khác biệt. Điều làm khác biệt giữa chúng là các trang lá (leaf pages). Tùy thuộc vào số lượng entries có cùng key thì GIN sẽ có cơ chế:\nPosting list: nếu số lượng row có cùng key còn ít, thì leaf page là một trang chứa posting list. Key bây giờ là duy nhất trên leaf page. Index tuple sẽ không trỏ trực tiếp đến các rows tương ứng mà sẽ trỏ đến một posting list là danh sách chứa các con trỏ (TIDs) cho tất cả các row có chứa giá trị tương ứng. Khi insert một row mới mà đã có key entry trên cây thì nó chỉ cần tìm đến entry và thêm TID của row mới vào posting list, làm như vậy sẽ không chia trang trên cây nữa. Chú ý rằng các items trong posting list sẽ được sắp xếp tăng dần theo TID.\nPosting tree: Tuy đã lưu dạng entry tuple, nhưng khi có quá nhiều row cùng giá trị (entry tuple nhiều trong 1 trang) thì cây riêng lại được tạo ra để chứa các TIDs - cây này gọi là posting tree.\nPosting tree cũng là một cấu trúc B-Tree riêng biệt được lưu trữ trên các trang của GIN index, trong trường hợp này TID của row sẽ được dùng làm key (TID vẫn được sắp xếp như trên posting list).\nCó nhiều posting tree trong một GIN index, mỗi cây đại diện cho một giá trị (key) cụ thể. Và một row trong table có thể được tham chiếu bởi nhiều TID được lưu ở các posting list/tree khác nhau.\nÁp dụng vào một dự án thực tế sẽ như thế nào? Cách mình thường hay dùng FTS này là mình sẽ tạo một column để chứa giá trị tsvector (đánh trọng số nếu cần) và đánh index trên nó. Sau đó tạo trigger để cập nhật field khi INSERT và UPDATE. Sau đó mọi thứ mình chỉ cần query trên column này một cách nhanh chóng. PostgreSQL\u0026rsquo;s full text search so với một số giải pháp khác Tùy vào nhu cầu tìm kiếm, resources mà chúng ta có những lựa chọn như Elastic, Algolia hay full text search của postgresql. Tuy nhiên với mình, những project có quy mô vừa và nhỏ thì mình thường dùng Full text search trong postgresql hơn vì:\nKhông cần đến phần mềm hay thư viện nào khác. Không cần thêm server riêng cho module tìm kiếm. Dùng chính database sử dụng cho application để query tìm kiếm. Nên sẽ dễ kiểm soát và không cần quan tâm đến vấn đề đồng bộ so với giải pháp như Elastic. References Chapter 12. Full Text Search Full-Text Search in PostgreSQL GIN – JUST A KIND OF INDEX Full text search in milliseconds with PostgreSQL Full-Text Search trong PostgreSQL - Phần 1 Additional Supplied Modules ","id":1,"section":"posts","summary":"\u003cp\u003e\u003cimg src=\"/img/posts/technique/postgresql/search.png\" alt=\"full text search\"\u003e\u003c/p\u003e\n\u003ch3 id=\"giới-thiệu-về-full-text-search\"\u003eGiới thiệu về Full-Text Search\u003c/h3\u003e\n\u003cp\u003eCó lẽ chúng ta đều biết và  sử dụng qua một kỹ thuật tìm kiếm rất cơ bản, đó là thông qua câu lệnh \u003cstrong\u003eLIKE\u003c/strong\u003e của SQL.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-sql\" data-lang=\"sql\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eSELECT\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003ecolumn_name\u003c/span\u003e(s)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eFROM\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003etable_name\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003eWHERE\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003ecolumn_name\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003eLIKE\u003c/span\u003e pattern;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eSử dụng \u003cstrong\u003eLIKE\u003c/strong\u003e, chúng ta chỉ tìm kiếm ở column đã định trước, do đó không gian tìm kiếm bị giới hạn hơn. Về tốc độ, câu lệnh LIKE cũng tương đương với lệnh chúng ta\nmatching pattern cho từng chuỗi của từng rows của column tương ứng thì bạn cũng hiểu tốc độ sẽ như thế nào rồi. Kết quả tìm kiếm có độ nhiễu cao và gặp vấn đề về từ đồng nghĩa.\u003c/p\u003e","tags":["Technique","PostgreSQL"],"title":"Full-Text Search trong PostgreSQL","uri":"https://lhsang.github.io/posts/technique/full-text-search-postgresql/","year":"2020"},{"content":"Từ ghế học đường bước vào môi trường doanh nghiệp là một sự thay đổi không hề nhẹ về thói quen, cách làm việc và kể cả kiến thức mình đã được học. Những thứ trong trường chỉ là những thứ rất cơ bản, chưa đủ đáp ứng cho công việc công ty. Mặc dù làm một trang web, mobile application hay một desktop application hoàn chỉnh thì mình cũng đủ sức để làm ra, nhưng vấn đề tại sao vẫn chưa đáp ứng được cho môi trường doanh nghiệp?\nThói quen code chỉ để cho chạy: code cho chạy không quan tâm đến performance bởi vì application mình code thì chỉ đâu đó mình dùng và bạn bè test qua thì dữ liệu rồi tải lượng cũng không đáng kể để mình chưa thấy được vấn đề performance quan trọng như thế nào. Hơn nữa, khi đọc docs thì cũng hay đọc những thứ mình cần tức thì chứ không quan tâm xung quanh, hoạt động như nào và sâu xuống có gì, làm theo =\u0026gt; code chạy =\u0026gt; xong :v Chưa có tầm nhìn để mở rộng: thường thì đồ án trong trường thì mình cũng như nhiều bạn sẽ code sao cho nhanh cho dễ cho mình thời điểm đó vì đồ án chỉ chấm xong là \u0026ldquo;vứt\u0026rdquo;, chứ đâu cần mở rộng. Vẫn nghĩ rằng nếu muốn mở rộng một chức năng tương tự thì chỉ cần copy ra sửa một chút là xong. Nhưng giờ nhìn lại hơi \u0026ldquo;vô học\u0026rdquo; khi chúng ta đã được học rất nhiều kiến trúc, kỹ thuật để lập trình và cả lập trình hướng đối tượng. Thói quen code mình mình đọc: code mọi thứ riêng biệt, tổ chức thư mục chưa tối ưu theo một pattern nào, gán cứng nhiều giá trị trong code hay đặt tên biến không nhất quán, code liên tiếp một đoạn dài ko phân cách nhóm code ra vì nghĩ mình có thể control được. Có thể là thời điểm đó là đúng với điều mình nghĩ, nhưng sau này thì mình không chắc là mình có thể đọc lại đống code mà chính mình viết ra. Sợ sai sợ bị chửi: thực tập sinh mà, khi bước vào công ty có lẻ rằng mình là nhỏ tuổi non kinh nghiệm nhất rồi, nên khi hỏi hay bày tỏ quan điểm thường cân nhắc rất kĩ trước là có nên nói ra vì chưa tự tin vào ý kiến, quan điểm của mình nên nhiều lúc thôi nghĩ im trất cho lành :v hoặc đi hỏi riêng một bạn cùng cấp cho đỡ nhục đỡ bị chửi thay vì bày tỏ quan điểm trước những buổi họp, daily meeting trước team. Và còn rất nhiều vấn đề nhỏ nhặt khác, tuy nhiên nhiêu đó đã đủ ảnh hưởng nhiều đến công việc của mình, của team rồi và của công ty rồi. Sau một kỳ thực tập vài tháng thì mình mới nhìn nhận được những thói quen như vậy và đã được rèn luyện để tốt hơn ở đây. Kiến thức là cái quan trọng, tuy nhiên kiến thức rộng thôi chưa đủ, phải đủ sâu và phải đủ tầm. Nên những điều nhỏ nhặt này mình cho rằng rất quan trọng để mình tối ưu và hoàn thiện bản thân hơn. Nên mình sẽ chia sẻ nhẹ vài điều mình thu được từ kỳ thực tập ngắn ngủi theo hệ quy chiếu của mình.\nThói quen đọc document tốt hơn khi thường xuyên được giao nhiệm vụ research liên quan đến tech stack của công ty và thậm chí những vấn đề mới để sharing lại cho team. Khác với trước, khi đọc docs thì mình đọc mình biết thì bấy giờ mình đọc để làm việc cho công ty và đương nhiên khi được review code thậm chí thỉnh thoảng lại bị hỏi ngang, không biết sẽ bị ăn chửi ngay. Nhưng nhờ ăn chửi mà mình lại phải đọc thật kỹ trước khi đặt tay vào cào phím. Sharing chỉ đơn giản là một lời chia sẻ ngắn trong buổi daily meeting hay buổi thuyết trình cũng vậy, khi nói cho người khác nghe thì đâu đó tâm lý buộc mình phải chắc chắn về lời mình nói ra. Học lém được thói quen đọc code ngay từ trong core lib/framework. Trước giờ khi không biết lib/framework này có hàm gì, nhận tham số gì và hoạt động như nào thì toàn lên google để tìm vì cho rằng code trong core thường rối, nhưng không, các thư viện/framework thường được ghi chú và code rất rõ ràng (đôi lúc mình còn thấy ghi chú, giải thích code còn nhiều gấp 10 lần số lượng dòng code) và docs trên các trang web đó cũng thưuòng lấy từ đây ra cả ấy mà :v. Đọc code trong core là cách mình thấy học được rất nhiều: lib đó có những thuộc tính nào, hàm nào, hoạt động như thế nào và hơn nữa là học được phong cách tổ chức code theo mình là thấy khá chuẩn và tối ưu. (Nhưng cũng tùy ngôn ngữ, các ngôn ngữ bậc thấp như C thì mình cùng công nhận rằng code nhìn vào hơi loạn :\u0026rsquo;( )\nCode làm sao cho tối ưu vì dữ liệu doanh nghiệp không phải vài ngàn records như đồ án mình làm, code \u0026ldquo;dỏm\u0026rdquo; chỉ cần đưa lên môi trường staging là chậm ngó thấy ngay. Nhưng trước khi đưa lên các môi trường thì phải trải qua 1,2 lần review code từ các anh senior. Đặt tên biến, gán cứng giá trị trong code, hay code một lều không phân đoạn để một đống thì ăn chửi là điều đương nhiên, code đâu phải mình mày đọc và mày với tao đâu phải làm việc đây suốt đời mà còn lớp khác vào nữa, sau vài tháng ăn chửi sẽ khiến mình tốt hơn, có lúc bị chửi đến mức tự ái nhưng giờ nhìn lại phải biết ơn những câu chửi đó ^^\nĐể join và làm được task trong project thì mình cũng thường được cho thời gian để đọc full project. Một project không hề nhỏ, code người khác viết, mình thì mới biết cơ bản làm sao hiểu được luồng đi của ứng dụng? Đâu đó được chỉ debug để nắm luồng chạy của ứng dụng - một cách mà mình chưa từng làm trước đây (vì thường đọc code của chính mình viết hoặc bạn viết thì hỏi bạn :)) Khá khoai nhưng rất lợi hại, đỡ phải chạy hỏi cùng với sự bỡ ngỡ vì chính mình có thể thấy được luồng chạy. Quá trình đọc project thì mình cũng để ý cách tổ chức thư mục cũng như các class rất khoa học (khoa học sẽ tốn thời gian hơn, nhưng mở rộng và maintain rất tiện và nhanh chóng). Và được bonus thêm những lời giải thích tại sao dùng pattern này, làm vậy sau này sẽ có lợi gì.\nMay mắn được vào team anh chị cũng rất tâm lý, rất biết cách dìu dắt sinh viên, lúc đầu bày tỏ ý kiến luôn sợ sai, sợ thiếu nhưng dần dần được nhắc nhở và khơi gợi ý kiến thì mình cũng không ngại bày tỏ quan điểm trong các cuộc họp mặc dù chỉ là một thực tập sinh nhỏ nhoi. Sai thì nói ra mới biết sai mà sửa không thì cứ sai mãi :v Tuy rằng mạnh dạn chia sẻ ý kiến nhưng cũng không quên rằng đừng quá tự tin vào bản thân mà bị mọi người có cái nhìn không đẹp :v\nThời điểm này, những thói quen như vậy mình coi trọng hơn là kiến thức mình học được rất nhiều, vì đấy là sẽ là những tiền đề để kiến thức, thái độ của mình xa hơn, sâu hơn trong tương lai. Trên là những vấn đề mình gặp phải và được cải thiện sau vài tháng tham gia vào môi trường doanh nghiệp, hy vọng đâu đó ít nhiều giúp được các bạn chưa đi làm hoặc đi làm mà vẫn những thói quen \u0026ldquo;chưa tốt\u0026rdquo; đó sẽ có lại cái nhìn tốt hơn, rộng hơn.\n","id":2,"section":"posts","summary":"\u003cp\u003eTừ ghế học đường bước vào môi trường doanh nghiệp là một sự thay đổi không hề nhẹ về thói quen, cách làm việc và kể cả kiến thức mình đã được học. Những thứ trong trường chỉ là những thứ rất cơ bản, chưa đủ đáp ứng cho công việc công ty. Mặc dù làm một trang web, mobile application hay một desktop application hoàn chỉnh thì mình cũng đủ sức để làm ra, nhưng vấn đề tại sao vẫn chưa đáp ứng được cho môi trường doanh nghiệp?\u003c/p\u003e","tags":["Internship","Experience"],"title":"Ngoài kiến thức thì mình còn học được những gì trong kỳ thực tập?","uri":"https://lhsang.github.io/posts/experience/ngoai-kien-thuc-minh-con-hoc-duoc-gi-khi-thuc-tap/","year":"2020"},{"content":"Ngay thời điểm chuẩn bị để đi phỏng vấn một công ty mới thì mình ngồi hệ thống lại một số câu hỏi mà mình đã được hỏi trong các cuộc phỏng vấn trước đây và những câu hỏi basic nhất mà mình nghĩ nhà tuyển dụng sẽ hỏi thì sẵn tiện mình release luôn bài này khi cần thì coi lại để chuẩn bị tốt nhất trong buổi phỏng vấn. Mình vẫn là sinh viên, đến thời điểm này mình đã đi phỏng vấn ở 2 công ty (tuy nhiên bạn bè mình có mặt ở khá nhiều công ty nên kinh nghiệm phỏng vấn thì mình cũng học được rất nhiều họ) và mình chỉ chia sẻ những thứ thuộc phạm trù của mình nên thiếu sót thì sẽ không tránh khỏi. Nếu có gì hay cần bổ sung mời các bạn comment xuống dưới, mình sẵn sàng tranh luận và tiếp thu :D\nBài này thực tế cần hơn khi đi phỏng vấn ở một số công ty mà họ không quan tâm technical skills của bạn về thứ cụ thể mà họ đang làm, có nghĩa là họ cần một người vững kiến thức nền và sau đó họ sẽ đào tạo lại. Nhưng dù là vị trí cao thấp hay công ty nhỏ lớn gì thì những kiến thức như vậy mình nghĩ chúng ta cần nắm vững, không phải chỉ để trả lời phỏng vấn mà là nó rất cơ bản chúng ta phải biết nếu học ngành này.\nThường thì các công ty có quy trình, chương trình rõ ràng cho intern, fresher thì sẽ chia ra ít nhất 2 vòng với 2 loại câu hỏi (Culture fit - Technical)\nCulture fit interview Đơn giản culture fit là một buổi phỏng vấn về văn hoá và giao tiếp, nơi cả công ty và ứng viên cùng check xem liệu mình và đối phương có phải một “good match” hay không. Phần này sẽ được phòng nhân sự hỏi, có thể được hỏi trong một buổi riêng hoặc hỏi chung ở đầu, sau khi phỏng vấn technical.\nNhà tuyển dụng (NTD) sẽ có một số câu hỏi hoặc đặt một tình huống hỏi mình giải quyết như thế nào. Tất cả những gì chúng ta cần là chuẩn bị một tâm thế chân thành và thoải mái để vượt qua phần này. Sau 2 lần đi phỏng vấn thì rút ra một số câu mình được hỏi sẽ rơi vào những phạm trù sau:\nTeamwork Management Motivation Desired working culture Chẳng hạn:\nKhi làm việc nhóm em thường gặp những vấn đề gì khó khăn và em đã giải quyết như thế nào? Khi được giao một công việc, bạn sẽ hoàn thành nó như thế nào? Bạn sẽ làm gì khi không thể hoàn thành deadline? Kể một lần sai lầm và bị chỉ trích, bạn phản ứng về điều đó như thế nào? Mức lương mong muốn của em là bao nhiêu cho vị trí này? Đừng quên chuẩn bị một bài giới thiệu thật hoành tráng (tôi là ai, điểm mạnh điểm yếu như nào, mục tiêu định hướng rõ ràng, \u0026hellip;)\nTechnical Phần này công ty có thể sẽ cho chúng ta test trước 1 bài trắc nghiệm hay code trên một số trang luyện thuật toán để sàng lọc rôì mới gặp phỏng vấn trực tiếp. Với mức độ bài test ở mức độ trung bình, đạt \u0026gt; range(50%, 70%) là pass (tùy). So với kiến thức hỏi trực tiếp thì bài test hỏi rộng hơn do là trắc nghiệm, rộng nhưng không sâu.\nPhỏng vấn technical trực tiếp thì có thể có 1, 2, 3 người, tùy công ty. Kiến thức sẽ rơi vào những môn được học từ năm nhất tới năm 3, có thể hỏi vài câu thực tế liên quan đến vị trí mình apply hoặc liên quan đến công việc cũ của mình (nếu có):\nCấu trúc dữ liệu và thuật toán: sẽ hỏi những câu cơ bản và kèm theo một câu hỏi ứng dụng, hoặc so sánh 2 cấu trúc liên quan.\nMảng, danh sách liên kết (đơn, đôi vòng) Stack \u0026amp; queue Sort, search. Tree (binary, AVL, B-tree) Graph Đệ qui, quy hoạch động Cấu trúc heap, hashtable, hashmap Lập trình hướng đối tượng:\n4 tính chất Phân biệt interface vs abstract, class vs object destructor and constructor design pattern (nhóm, biết những loại nào, khi nào dùng, ví dụ trong thực tế) Cơ sở dữ liệu:\nindexes (cơ chế, các loại index) Delete và Truncate Transaction (các thuộc tính, bài toán thực tế liên quan như 2 user cùng đặt 1 sp nhưng chỉ còn 1 sp) DDL - DML - DCL - TCL. What is DDL and DML? Câu hỏi ngẫu nhiên về version và điểm mạnh điểm yếu các csdl hay viết một truy vấn để lấy record cao thứ n từ bảng chẳng hạn. Mạng máy tính:\nTCP vs UDP (so sánh, ứng dụng) HTTP vs HTTPS DNS, DHCP LAN, WAN, MAN Hệ điều hành:\nCác thuật toán lập lịch Thread vs process Race condition Deadlock vs lock Một số lệnh cơ bản linux Vùng nhớ heap và stack Kiến thức back-end:\nCác phương thức trong RESTful Các thuật toán cache Cookie, session, localstorage Một số kỹ thuật bảo mật Ít nhiều nên có kiến thức về kiến trúc Mình chỉ liệt kê ra nội dung NTD sẽ hỏi để tham khảo đương nhiên tương ứng với mỗi câu trả lời của bạn thì NTD sẽ bổ sung và hỏi sâu theo hướng nào, bạn cố gắng tìm hiểu, hệ thống lại và trả lời một cách đầy đủ, thông minh nhất có thể.\nTrên là những phần mình đã được hỏi khi đi phỏng vấn hy vọng ít nhiều cũng giúp được các bạn đạt được vị trí mong muốn ở công ty mình ước ao \u0026lt;3\nGood luck!\n","id":3,"section":"posts","summary":"\u003cp\u003eNgay thời điểm chuẩn bị để đi phỏng vấn một công ty mới thì mình ngồi hệ thống lại một số câu hỏi mà mình đã được hỏi trong các cuộc phỏng vấn trước đây và những câu hỏi basic nhất mà mình nghĩ nhà tuyển dụng sẽ hỏi thì sẵn tiện mình release luôn bài này khi cần thì coi lại để chuẩn bị tốt nhất trong buổi phỏng vấn. Mình vẫn là sinh viên, đến thời điểm này mình đã đi phỏng vấn ở 2 công ty (tuy nhiên bạn bè mình có mặt ở khá nhiều công ty nên kinh nghiệm phỏng vấn thì mình cũng học được rất nhiều họ) và mình chỉ chia sẻ những thứ thuộc phạm trù của mình nên thiếu sót thì sẽ không tránh khỏi. Nếu có gì hay cần bổ sung mời các bạn comment xuống dưới, mình sẵn sàng tranh luận và tiếp thu :D\u003c/p\u003e","tags":["Experience","Interview"],"title":"Sinh viên IT \"bị\" hỏi những gì trong những lần đi phỏng vấn vị trí intern, fresher?","uri":"https://lhsang.github.io/posts/experience/intern-fresher-ntd-hoi-gi/","year":"2020"},{"content":"\n1. What is Nginx? Nginx ban đầu được tạo ra như một máy chủ web để giải quyết vấn đề C10k (là một vấn đề liên quan đến vấn đề hiệu suất xử lý 10.000 kết nối cùng lúc), nhưng bây giờ với các tính năng mở rộng Nginx cũng được sử dụng phổ biến như một máy chủ proxy (reverse proxy server), HTTP cache hoặc dùng làm cân bằng tải (load balancer). Nginx được thiết kế khả năng chịu tải đồng thời cao và tốc độ cực nhanh. Nginx cấu hình dễ dàng hơn so với Apache httpd Reverse proxy khi có nhiều web services listen trên nhiều port và cần một single public endpoint để định tuyến lại internal requests (cho phép dùng nhiều domain trên port 80). 2. Nginx architecture. Kiến trúc cơ bản của nginx bao gồm 1 master process và các workers. master process thực hiện các hoạt đông đặc quyền như đọc config và binding đến các port, sau đó tạo ra các processes con (3 loại process con). cache loader process (load cache) chạy khi khởi động để load bộ đệm trên đĩa vào RAM rồi exit. Nó được lên kế hoạch hoạt động, nên nhu cầu resource rất thấp. cache manager process (quản lý cache) chạy định kỳ và chia các mục từ cache trên đĩa để giữ chúng vừa với kích thước được cấu hình. worker processes xử lý network connections, read và write vào đĩa và liên lạc với các upstream server. Mỗi worker process là một single-thread và chạy độc lập với nhau, chúng liên lạc với nhau thông qua bộ nhớ dùng chung cho dữ liệu cache dùng chung, dữ liệu lưu phiên và các tài nguyên dùng chung khác. 3. Nginx sử dụng single thread NGINX sử dụng cơ chế asynchronous và event-driven để handle tất cả các connection. Để đạt được điều này nginx hoạt động trên các socket ở chế độ non-blocking và sử dụng một số phương pháp khác như epoll và kqueue. Đây là điểm khác biệt của nginx so với một số server khác, nginx có thể handle đồng thời hàng triệu request và khả năng scale rất tốt.\nTuy nhiên vấn đề asynchronous và event-driven vẫn có những problem, nếu nhiều module thứ 3 hoặc chính dev sử dụng blocking thì nginx mất đi thế mạnh vốn có.\n4. Giải quyết vấn đề blocking. to be continue\n5. Thử load balancing dùng nginx. Mô tả ngắn:\nChạy web app(flask) trên 2 server (demo dùng 2 port). Dùng nginx để load balancing trên 2 server. Let\u0026rsquo;s start:\nĐiều chỉnh tường lửa allow cho Nginx HTTP (port 80) $ sudo ufw allow \u0026#39;Nginx HTTP\u0026#39; Verify Demo flask app from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/index\u0026#39;) def demo(): return \u0026#39;hello world\u0026#39; Config nginx (/etc/nginx/nginx.conf) upstream demo { server 127.0.0.1:8080; server 127.0.0.1:8000; } server { listen 80; location / { proxy_pass http://demo; } } Reload service nginx. $ sudo service nginx reload Chạy web app trên port 8080 và 8000 giống như đã config $ flask run --port=8080 $ flask run --port=8000 Thuật toán load balancing mặc định của Nginx là Round robin(các máy chủ sẽ được lựa chọn tuần tự, vòng tròn), chúng ta có thể config lại một thuật toán khác nếu muốn.\nKết quả.\nLần đầu request được điều hướng tới server với port 8080 Lần request tiếp theo được điều hướng đến server port 8000 References Nginx Tutorial #1: Basic Concepts Nginx Tutorial An Introduction to NGINX for Developers Inside NGINX: How We Designed for Performance \u0026amp; Scale nginx How to configure load balancing using Nginx Thread Pools in NGINX Boost Performance 9x! ","id":4,"section":"posts","summary":"\u003cp\u003e\u003cimg src=\"/img/posts/technique/nginx/nginx.png\" alt=\"Nginx\"\u003e\u003c/p\u003e\n\u003ch3 id=\"1-what-is-nginx\"\u003e1. What is Nginx?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNginx ban đầu được tạo ra như một máy chủ web để giải quyết vấn đề \u003cstrong\u003eC10k\u003c/strong\u003e (là một vấn đề liên quan đến vấn đề hiệu suất xử lý 10.000 kết nối cùng lúc), nhưng bây giờ với các tính năng mở rộng Nginx cũng được sử dụng phổ biến như một máy chủ proxy (reverse proxy server), HTTP cache hoặc dùng làm cân bằng tải (load balancer).\u003c/li\u003e\n\u003cli\u003eNginx được thiết kế khả năng chịu tải đồng thời cao và tốc độ cực nhanh. Nginx cấu hình dễ dàng hơn so với \u003cstrong\u003eApache httpd\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eReverse proxy khi có nhiều web services listen trên nhiều port và cần một single public endpoint để định tuyến lại internal requests (cho phép dùng nhiều domain trên port 80).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-nginx-architecture\"\u003e2. Nginx architecture.\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eKiến trúc cơ bản của \u003cstrong\u003enginx\u003c/strong\u003e bao gồm \u003cstrong\u003e1 master process\u003c/strong\u003e và \u003cstrong\u003ecác workers\u003c/strong\u003e.\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003emaster process\u003c/strong\u003e thực hiện các hoạt đông đặc quyền như đọc config và binding đến các port, sau đó tạo ra các processes con (3 loại process con).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecache loader process\u003c/strong\u003e (load cache) chạy khi khởi động để load bộ đệm trên đĩa vào RAM rồi exit. Nó được lên kế hoạch hoạt động, nên nhu cầu resource rất thấp.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecache manager process\u003c/strong\u003e (quản lý cache) chạy định kỳ và chia các mục từ cache trên đĩa để giữ chúng vừa với kích thước được cấu hình.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eworker processes\u003c/strong\u003e xử lý network connections, read và write vào đĩa và liên lạc với các upstream server. Mỗi worker process là một single-thread và chạy độc lập với nhau, chúng liên lạc với nhau thông qua bộ nhớ dùng chung cho dữ liệu cache dùng chung, dữ liệu lưu phiên và các tài nguyên dùng chung khác.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/img/posts/technique/nginx/architecture.png\" alt=\"Architecture\"\u003e\u003c/p\u003e","tags":["Technique","Nginx"],"title":"Nginx là gì ? Demo load balancing với nginx","uri":"https://lhsang.github.io/posts/technique/nginx/","year":"2020"},{"content":"gRPC là gì ? Trong thời điểm hiện tại thì JSON REST API vẫn đang rất phổ biến và phổ thông bởi tính dễ sử dụng. Tuy nhiên để nâng cao hiệu năng cho trang web, chúng ta sẽ tìm hiểu vê một framework RPC mới dựa trên protocol buffers và HTTP/2 của Google có tên là gRPC\ngRPC là một RPC platform được phát triển bởi Google nhằm tối ưu hoá và tăng tốc việc giao tiếp giữa các service với nhau trong kiến trúc microservice.\ngRPC dùng Protocol Buffers giảm kích thước request và response data, RPC để đơn giản hoá trong việc tạo ra các giao tiếp giữa các service với nhau và dùng HTTP/2 để tăng tốc gửi/nhận HTTP request.\n1. RPC là gì?\nRPC là từ viết tắc của Remote Procedure Call, nó được xây dựng với ý tưởng là đơn giản hoá việc giao tiếp giữa những service với nhau, thay vì những service giao tiếp với nhau theo kiểu RESTful API thì giờ đơn giản là gọi hàm như những object nói chuyện với nhau thôi, còn việc phân tán các service là chuyện của tương lai không dính liếu đến việc code.\n2. HTTP/2\nHTTP/2 là một phiên bản nâng cấp của HTTP/1.1, HTTP/2 sinh với với mục đích cải thiện tốc độ giao tiếp giữa client/server trên nền tảng Web.\nHTTP/1.1 có nhược điểm là xử lý từng request 1, tức là một request nào đó gởi đến server xử lý xong thì mới gởi request khác đi. Ví dụ khi trang web cần lấy 2 file script.js và style.css với thứ tự tương ứng thì file script.js được trả về xong thì server mới xử lý yêu cầu lấy file style.css. Điều này làm cho thời gian chờ để lấy về tất cả dữ liệu rất tốn thời gian.\nHTTP/2 được ra đời với các mục tiêu: cải thiện tốc dộ giao tiếp, giữ được tính tương thích với HTTP/1và cho phép cả trình duyệt lẫn máy chủ có thể chọn loại giao thức kết nối:\nCompression(nén) of request headers Mỗi request của HTTP sẽ mang rất nhiều data headers đi và đến cho dù nó giống nhau từ request thứ 2 trở đi. Vì cả client và server đều duy trì một danh sách các headers được sử dụng cho các request trước đó (header frames) nên HTTP/2 sẽ loại bỏ những data headers trùng lặp lại ở những lần request thứ 2 trở đi và nén headers trước khi gửi đi (sử dụng HPACK), sau đó HPACK sẽ tìm kiếm lại header trong header frame để xây dựng lại headers đầy đủ.\nBinary protocol Browsers sẽ convert text sang binary trước khi gởi qua đường network. Mục đích nhằm giảm kích thước gói tin, tiết kiệm băng thông, giảm chi phí parsing data.\nRequest multiplexing over a single TCP connection HTTP/2 có thể gửi cùng lúc nhiều request đến qua một kết nối TCP(single TCP) và kết quả được trả về bất đồng bộ với nhau (chú ý là hầu hết trình duyệt cũng có giới hạn TCP connections đến một server).\nHTTP/2 Server Push Thêm một cách để tối ưu tốc độ loading của website, thay vì phải có request từ client thì server mới trả resource liên quan về, HTTP/2 sẽ hiểu những resource liên quan và đẩy resource về cho client luôn mà không cần client gửi request. 3. Vậy còn Protocol Buffers ?\nProtocol Buffers gọi tắt là protobuf là ngôn ngữ được phát triển bới Google tạo ra chuẩn sắp xếp mã hóa data với mục đích tạo buffer truyền và nhận giao tiếp một cách linh hoạt, hiệu quả, đơn giản và tăng tốc độ truyền nhận (có vẻ khá giống XML hoặc JSON). Nó lưu trữ dữ liệu có cấu trúc có thể được Serialize hoặc De-Serialized tự động bưởi nhiều ngôn ngữ khác nhau.\nChỉ cần định nghĩa cấu trúc data sử dụng protobuf tạo buffer, sau đó chỉ việc sử dụng hàm thư viện protobuf dễ dàng ghi và đọc cấu trúc data đó đến và từ rất nhiều ngôn ngữ khác nhau. Có thể update lại cấu trúc data nhưng code chương trình sẽ không cần phải thay đổi.\nSo sánh một chút với XML và JSON:\nProtobuf JSON XML Không dành cho người đọc vì là binary Con người có thể đọc và chỉnh sửa dễ dành Con người có thể có thể đọc và chỉnh sửa dễ dàng Khó decode mà không biết schema, định dạng dữ liệu không rõ ràng Có thể phân tích mà không cần biết schema Có thể phân tích cú pháp mà không cần biết schema Xử lý rất nhanh, nhỏ hơn 3 - 10 lần so với XML hoặc JSON - - Protobuf rất nhanh nhưng có những tình huống không nên sử dụng nó:\nKhi cần hoặc muốn dữ liệu con người có thể đọc dễ dàng. Dữ liệu từ Service được sử dụng trực tiếp bởi Browser. Demo gRPC using python gRPC Basics - Python\nReferences HTTP/2: the difference between HTTP/1.1, benefits and how to use it Protocol Buffers - Developer Guide gRPC Basics - Python Các kỹ sư Eureka đã tối ưu ứng dụng chat sử dụng gRPC như thế nào gRPC official docs [Web] HTTP2 Protocol buffers là gì và những điều căn bản cần biết về nó ","id":5,"section":"posts","summary":"\u003ch2 id=\"grpc-là-gì-\"\u003egRPC là gì ?\u003c/h2\u003e\n\u003cp\u003eTrong thời điểm hiện tại thì \u003cstrong\u003eJSON REST API\u003c/strong\u003e vẫn đang rất phổ biến và phổ thông bởi tính dễ sử dụng. Tuy nhiên để nâng cao hiệu năng cho trang web, chúng ta sẽ tìm hiểu vê một framework RPC mới dựa trên \u003cstrong\u003eprotocol buffers\u003c/strong\u003e và HTTP/2 của \u003cstrong\u003eGoogle\u003c/strong\u003e có tên là \u003cstrong\u003egRPC\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003egRPC\u003c/strong\u003e là một \u003cstrong\u003eRPC\u003c/strong\u003e platform được phát triển bởi Google nhằm tối ưu hoá và tăng tốc việc giao tiếp giữa các service với nhau trong kiến trúc microservice.\u003c/p\u003e","tags":["Technique","RPC","HTTP"],"title":"Tìm hiểu gRPC","uri":"https://lhsang.github.io/posts/technique/grpc/","year":"2020"},{"content":"\n1. Message-boker ?\nLà chương trình đóng vai trò trung gian lưu trữ cũng như điều phối (valadating, transforming, routing messages) các yêu cầu (message) giữa sender và reciever. Mesage-boker có 2 hình thức giao tiếp cơ bản là: Publish và Subscribe (Topics) Point-to-Point (Queues) 2. RabbitMQ là gì ?\nRabbitMQ là một message boker (message-oriented middleware) hay còn gọi là phần mềm quản lý hàng đợi message (thường được gọi là môi giới message hay trình quản lý message). Nói đơn giản đây là phần mềm định nghĩa hàng đợi một ứng dụng khác có thể kết nối tới để bỏ message vào và gửi message dựa trên nó.\nMessage: ở đây có thể chứa nhiều kiểu thông tin. Ví dụ như thông tin về một process/task để khởi động một ứng dụng nào đó (nằm trên một server khác), hoặc có thể là một message chứa text đơn giản.\nRabbitMQ hỗ trợ nhiều giao thức (AMQP, STOMP, MQTT, HTTP and Websockets), tuy nhiên phương thức phố biến nhất mà rabbitmq sử dụng là AMQP - Advanced Message Queue Protocol\nPhần mềm quản lý hàng đợi chứa các message cho đến khi ứng dụng nhận đến lấy message. Một số thuật ngữ trong RabbitMQ Producer: Bên phát hành message (publisher) Consumer: Bên nhận tin (subscriber) Exchange: Làm nhiệm vụ điều hướng message từ producer đến các queue bên trong do các message không được công khai trực tiếp trong queue. Routing key: Là một khóa mà exchange dùng nó để quyết định cách đưa vào hàng đợi. Routing key có thể hiểu như một địa chỉ của message. Queues: có nhiệm vụ lưu trữ bản tin được gửi lên Connection: Là một kết nối TCP giữa ứng dụng của bạn và RabbitMQ Channel: Một channel là một kết nối ảo bên trong một connection. Khi bạn đẩy đi hoặc nhận các message từ hàng đợi, tất cả phải đi qua channel Binding: Là một kết nối giữa hàng đợi và exchange User: người dùng có thể kết nối đến #RabbitMQ bằng username/password. Mỗi người dùng được cấp quyền như đọc, ghi và cấu hình quyền bên trong một instance. User còn có quyền trên một host ảo. Virtual Host: Cung cấp chức năng tách ứng dụng dùng trên cùng #RabbitMQ. Người dùng khác nhau có quyền hạn khác nhau trên virtual host, hàng đợi hay exchange khác nhau. Chúng chỉ tồn tại trong một virtual host. 3. Tại sao dùng RabbitMQ ?\nProblems:\nĐối với các hệ thống sử dụng kiến trúc microservice thì việc gọi chéo giữa các service quá nhiều khiến luồng xử lý khá phức tạp. Mức độ trao đổi data giữa các thành phần tăng lên khiến cho việc lập trình trở nên khó khăn (maintain). Khi phát triển làm sao để dev tập trung vào business logic thay vì các công việc trao đổi ở tầng infrastructure. Với các hệ thống phân tán, khi việc giao tiếp giữa các thành phần đòi hỏi chúng phải biết nhau. Nhưng điều này rắc rối cho việc viết code. Một thành phần phải viết quá nhiều đâm ra rất khó maintain, debug Sử dụng RabbitMQ:\nTheo dõi được message và có thể retry (service ngừng hoạt động thì message vẫn còn trong queue). Một producer không cần phải biết comsumer. Nó chỉ việc gởi message đến các queue trong message-boker. Consumer chỉ việc đăng ký nhận message từ queue này. Vì producer giao tiếp với consumer trung gian qua message broker nên dù producer và consumer có khác biệt nhau về ngôn ngữ thì giao tiếp vẫn thành công.(Hiện nay rabbitmq đã hỗ trợ rất nhiều ngôn ngữ khác nhau). Một đặc tính của rabbitmq là bất đồng bộ(asynchronous). Producer không thể biết khi nào message đến được consumer hay khi nào message được consumer xử lý xong. Đối với producer, đẩy message đến message broker là xong việc. Consumer sẽ lấy message về khi nó muốn. Đặc tính này có thể được tận dụng để xây dựng các hệ thống lưu trữ và xử lý log. 4. Exchange\nMặc định exchange là chuỗi \u0026ldquo;\u0026rdquo;. Một exchange có thể có nhiều queue. Exchange có 4 loại:\nFanout: Một Fanout exchange sẽ đẩy message đến toàn bộ hàng đợi gắn với nó. Direct: Một Direct exchange sẽ đẩy message đến hàng đợi dựa theo khóa định tuyến – routing key (do producer khai báo). Ví dụ, nếu hàng đợi gắn với một exchange có binding key là pdfprocess, message được đẩy vào exchange với routing key là pdfprocess sẽ được đưa vào hàng đợi. Topic: Một topic exchange sẽ làm một lá bài (gọi là wildcard) để gắn routing key với một routing pattern khai báo trong binding Consumer có thể đăng ký những topic mà nó quan tâm. Cú pháp được sử dụng ở đây là * và #. Ví dụ: - booking.* -\u0026gt; Được đăng ký bởi tất cả những key với pattern bắt đầu bằng booking. - booking.# -\u0026gt; Được đăng ký bởi tất cả các key booking hoặc bắt đầu với booking Headers: Một header exchange sẽ dùng các thuộc tính header của message để định tuyến. Headers Exchange rất giống với Topic Exchange, nhưng nó định tuyến dựa trên các giá trị tiêu đề thay vì các khóa định tuyến. Dead Letter Exchange: Nếu không tìm thấy hàng đợi phù hợp cho tin nhắn, tin nhắn sẽ tự động bị hủy. RabbitMQ cung cấp một tiện ích mở rộng AMQP được gọi là “Dead Letter Exchange” — Cung cấp chức năng để chụp các tin nhắn không thể gửi được. 5. Workflow của RabbitMQ ?\nProducer đẩy message vào exchange. Khi tạo exchange, phải mô tả nó thuộc loại gì. Sau khi exchange nhận message, nó chịu trách nhiệm định tuyến message. Exchange sẽ chịu trách về các thuộc tính của message, ví dụ routing key, phụ thuộc loại exchange. Việc binding phải được tạo từ exchange đến hàng đợi. Trong trường hợp như ảnh, ta sẽ có hai binding đến hai hàng đợi khác nhau từ một exchange. Exchange sẽ định tuyến message vào các hàng đợi dựa trên thuộc tính của của từng message. Các message nằm ở hàng đợi đến khi chúng được xử lý bởi một consumer. Consumer xử lý message. 6. Thử bắn và nhận msg với rabbitmq (python)\nTạo một service gởi - sender.py: với queue tên \u0026lsquo;demo\u0026rsquo;, exchange \u0026rsquo;logs\u0026rsquo; đẩy msg đến toàn bộ queue (exchange_type=\u0026lsquo;fanout\u0026rsquo;). Bắn msg với routing_keys=\u0026lsquo;key1\u0026rsquo;, nội dung trong body import pika # using CloudAMQP (https://www.cloudamqp.com/) CLOUDAMQP_URL = \u0026#39;amqp://vvrzbnja:2xdFFhXnVM2o0QyeU6ynPHrgr9V5C8rK@woodpecker.rmq.cloudamqp.com/vvrzbnja\u0026#39; # establish a connection with RabbitMQ server. params = pika.URLParameters(CLOUDAMQP_URL) connection = pika.BlockingConnection(params) channel = connection.channel() # create queue with name \u0026#39;demo\u0026#39; channel.queue_declare(queue=\u0026#39;demo\u0026#39;) channel.exchange_declare(exchange=\u0026#39;logs\u0026#39;, exchange_type=\u0026#39;fanout\u0026#39;) # Ready to send a message channel.basic_publish(exchange=\u0026#39;logs\u0026#39;, routing_key=\u0026#39;key1\u0026#39;, body=\u0026#39;Hello world!\u0026#39;) print(\u0026#34; Sent message\u0026#34;) # close connection connection.close() Service nhận message- receiver.py. Đăng ký nhận msg ở queue \u0026lsquo;demo\u0026rsquo; import pika CLOUDAMQP_URL = \u0026#39;amqp://vvrzbnja:2xdFFhXnVM2o0QyeU6ynPHrgr9V5C8rK@woodpecker.rmq.cloudamqp.com/vvrzbnja\u0026#39; # Access the CLODUAMQP_URL environment variable and parse it (fallback to localhost) params = pika.URLParameters(CLOUDAMQP_URL) connection = pika.BlockingConnection(params) channel = connection.channel() # start a channel channel.queue_declare(queue=\u0026#39;demo\u0026#39;) # Declare a queue def callback(ch, method, properties, body): print(\u0026#34; [x] Received \u0026#34; + str(body)) channel.basic_consume(\u0026#39;hello\u0026#39;, callback, auto_ack=True) print(\u0026#39; [*] Waiting for messages:\u0026#39;) channel.start_consuming() connection.close() Run $ python sender.py $ python receiver.py References: RabbitMQ for beginners - What is RabbitMQ? Những điều cần biết về RabbitMQ RabbitMQ và Kafka\n","id":6,"section":"posts","summary":"\u003cp\u003e\u003cimg src=\"/img/posts/technique/rabbitmq/rabbitMQ.png\" alt=\"rabbitMQ\"\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e1. Message-boker ?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLà chương trình đóng vai trò trung gian lưu trữ cũng như điều phối (valadating, transforming, routing messages) các yêu cầu (message) giữa sender và reciever.\u003c/li\u003e\n\u003cli\u003eMesage-boker có 2 hình thức giao tiếp cơ bản là:\n\u003cul\u003e\n\u003cli\u003ePublish và Subscribe (Topics)\u003c/li\u003e\n\u003cli\u003ePoint-to-Point (Queues)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e2. RabbitMQ là gì ?\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRabbitMQ là một message boker (message-oriented middleware) hay còn gọi là phần mềm quản lý hàng đợi message (thường được gọi là môi giới message hay trình quản lý message).\nNói đơn giản đây là phần mềm định nghĩa hàng đợi một ứng dụng khác có thể kết nối tới để bỏ message vào và gửi message dựa trên nó.\u003c/p\u003e","tags":["Technique","Message broker","RabbitMQ"],"title":"RabbitMQ - Demo with python","uri":"https://lhsang.github.io/posts/technique/rabbitmq/","year":"2020"},{"content":"Từ một thằng sinh viên (năm 4) chỉ biết ăn chơi và học, bước ra ngưỡng cửa của xã hội để làm quen, học hỏi và kiếm tiền thì không tránh khỏi những thay đổi về thói quen, công việc,\u0026hellip; Có thể nói đây là lần đầu mình đi thực tập và đi kiếm tiền của thời sinh viên (trước đó có tham gia một startup tuy nhiên sẽ kể ở một câu chuyện khác), vì vậy mình sẽ kể về một ngày làm việc của mình - thực tập sinh công nghệ thông tin, cụ thể là python backend developer tại Teko Viet Nam.\nCông việc của mình là backend develop, ban đầu mình đảm nhiệm service message queue và hệ thống API nhưng cũng khá ít task (do hệ thống hoạt động ổn định đôi lúc cần hotfix, bổ sung hay refactor) và rảnh rỗi nên mình xin làm thêm phần report và connector. Đôi lúc không có task làm thì mình thường ngồi tìm hiều một công nghệ gì đấy rồi note lại (điển hình là các bài viết tại blog này - một số bài mình chưa public), hoặc chạy đi kiếm task, công ty product mà, không nhiều việc và deadline réo như các công ty outsourcing.\nCác công ty IT thường làm việc trễ, công ty mình 9h(nhưng cũng có thể linh hoạt sớm-trễ 1 tí) bắt đầu làm việc, nên tầm 7h30 thức giấc, vệ sinh và ăn sáng, 8h mình bắt đầu lên xe chạy đến công ty. Đến công ty tầm 9h, nhưng thực ra cũng chưa làm việc ngay, lên tới công ty mình thường ăn và uống thêm (công ty có chuẩn bị đồ ăn sáng, đồ ăn uống vặt luôn có và free) đến 9h15 tiến hành daily meeting để các thành viên trong team báo cáo hôm qua làm gì, có khó khăn, đề xuất giải pháp gì cho task đang làm và hôm nay dự định sẽ làm gì. Cũng chỉ là cuộc họp nội bộ nên rất cũng thỏa mái, vui vẻ trao đổi và đùa giỡn 1 tí lấy niềm vui cho ngày làm việc mới chứ không căng thẳng hay nghiêm túc như các cuộc họp khác =))\nDaily meeting xong (tầm 15p) thì về chỗ và tiến hành công việc thôi. Vì công việc có tính liên quan, nên trong quá trình làm việc mình cũng hay qua lại chỗ các anh chị để trao đổi và ngược lại, môi trường làm việc ở đây cũng thoả mái và vui vẻ, không nghiêm túc như các công ty khác, đặc biệt là công ty Nhật. Buổi sáng làm việc tới đúng 12h thì nhân viên được nghỉ đi ăn và nghỉ trưa. Mình thường cùng bạn bè hoặc anh/chị trong team đi ăn quán (công ty thỉnh thoảng cũng có đồ ăn trưa cho nhân viên) tầm 40p trở về công ty và làm vài ván game hay ngủ trưa rồi 13h30 lại tiếp tục công việc.\nBuổi chiều công việc vẫn vậy, tuy nhiên thời gian làm việc buổi chiều dài hơn sáng, nên mình thường làm việc một lúc rồi giải lao ăn uống gì 1 tí, đôi khi cả team tụ họp tại pantry để ăn uống trò chuyện. Hoặc căng thẳng quá thì ra phòng giải trí phóng phi tiêu hay chọc phá đồng nghiệp một tí (công ty mình nhân viên khá trẻ, đa số 9x nên nói chuyện đùa giỡn không vấn đề gì\u0026hellip;) rồi lại bàn tiếp tục.\nLàm việc đến 18h, nếu đói khát thì mình lại ăn =)) rồi ra về, đôi lúc công việc dang dở thì có thể ráng thêm cho xong, hay hết task sớm thì mình cũng vế sớm hơn 1 chút.\n18h về đến nhà tầm 19h, tắm rửa ăn uống, thỉnh thoảng cafe ăn uống với bạn bè rồi về lướt điện thoại máy tính gì rồi đi ngủ. Làm việc cả ngày dù vất vả hay không thì về nhà cũng thấy khá mệt và nhát nên cũng không muốn code hay học hành gì thêm. Hết ngày !!!!!\nMình cảm thấy khá may mắn khi những bước chân đầu cũng không quá khó khăn áp lực nhưng lại học được rất rất nhiều thứ nhờ được làm ở đây, nhìn chung thì môi trường công ty khá thỏa mái, công việc không áp lực, chính sách đãi ngộ mình cũng khá hài lòng. Nhưng cũng chỉ là bước đầu ra đời, cũng chỉ mới thực tập chưa nói lên được gì, cuộc sống và công việc sẽ còn nhiều thay đổi đang chờ mình.\nBàn làm việc của mình.\nLượn một vòng tầng mình đang làm việc lúc 18h15.\n","id":7,"section":"posts","summary":"\u003cp\u003eTừ một thằng sinh viên (năm 4) chỉ biết ăn chơi và học, bước ra ngưỡng cửa của xã hội để làm quen, học hỏi và kiếm tiền thì không tránh khỏi những thay đổi về thói quen, công việc,\u0026hellip; Có thể nói đây là lần đầu mình đi thực tập và đi kiếm tiền của thời sinh viên (trước đó có tham gia một startup tuy nhiên sẽ kể ở một câu chuyện khác), vì vậy mình sẽ kể về một ngày làm việc của mình - thực tập sinh công nghệ thông tin, cụ thể là python backend developer tại \u003cstrong\u003eTeko Viet Nam\u003c/strong\u003e.\u003c/p\u003e","tags":["Experience","internship"],"title":"Ngày làm việc ở công ty của một thực tập sinh IT","uri":"https://lhsang.github.io/posts/experience/mot-ngay-lam-viec-cua-thuc-tap-sinh-it/","year":"2020"}],"tags":[{"title":"Crawler","uri":"https://lhsang.github.io/tags/crawler/"},{"title":"Experience","uri":"https://lhsang.github.io/tags/experience/"},{"title":"HTTP","uri":"https://lhsang.github.io/tags/http/"},{"title":"Internship","uri":"https://lhsang.github.io/tags/internship/"},{"title":"Interview","uri":"https://lhsang.github.io/tags/interview/"},{"title":"Message Broker","uri":"https://lhsang.github.io/tags/message-broker/"},{"title":"Nginx","uri":"https://lhsang.github.io/tags/nginx/"},{"title":"PostgreSQL","uri":"https://lhsang.github.io/tags/postgresql/"},{"title":"RabbitMQ","uri":"https://lhsang.github.io/tags/rabbitmq/"},{"title":"RPC","uri":"https://lhsang.github.io/tags/rpc/"},{"title":"Technique","uri":"https://lhsang.github.io/tags/technique/"},{"title":"Thesis","uri":"https://lhsang.github.io/tags/thesis/"}]}